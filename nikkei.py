import scrapy
import sqlite3
from datetime import datetime
import os

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹
DB_PATH = os.path.abspath("/market_data.db")

class NikkeiSpider(scrapy.Spider):
    name = "nikkei"
    allowed_domains = ["www.nikkei.com"]
    start_urls = ["https://www.nikkei.com/markets/company/search/consensus/"]

    def __init__(self, target_date=None, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # target_dateã¯ scrapy runspider -a target_date=YYYYMMDD ã§å—ã‘å–ã‚‹
        if not target_date:
            raise ValueError("å®Ÿè¡Œæ™‚ã« -a target_date=YYYYMMDD ã®å½¢å¼ã§æ—¥ä»˜ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

        try:
            datetime.strptime(target_date, "%Y%m%d")
        except ValueError:
            raise ValueError("æ—¥ä»˜ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„")

        self.target_date = target_date

        # SQLite DBåˆæœŸåŒ–
        self.conn = sqlite3.connect(DB_PATH)
        self.cursor = self.conn.cursor()
        self.init_db()

        # å–å¾—ä»¶æ•°ã‚«ã‚¦ãƒ³ã‚¿
        self.total_seen = 0         # ãƒšãƒ¼ã‚¸ä¸Šã§è¦‹ã¤ã‘ãŸä»¶æ•°
        self.total_inserted = 0     # DBã«æ–°è¦ç™»éŒ²ã§ããŸä»¶æ•°

    def init_db(self):
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS consensus_url (
                target_date TEXT,
                code TEXT,
                name TEXT,
                nikkeiurl TEXT,
                quickurl TEXT,
                sbiurl TEXT,
                PRIMARY KEY (target_date, code)
            )
        """)
        self.conn.commit()

    def closed(self, reason):
        # çµ±è¨ˆã‚’è¡¨ç¤º
        msg = f"ğŸ“¦ å–å¾—ä»¶æ•°: {self.total_seen} ä»¶ / æ–°è¦ç™»éŒ²: {self.total_inserted} ä»¶ï¼ˆtarget_date={self.target_date}ï¼‰"
        self.logger.info(msg)
        print(msg)

        # ã‚¯ãƒ­ãƒ¼ã‚ºæ™‚ã«DBæ¥ç¶šã‚’é–‰ã˜ã‚‹
        self.conn.close()

    def parse(self, response):
        # ãƒšãƒ¼ã‚¸ã®è¡Œã‚’å–å¾—ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¯ code/name ãŒå–ã‚Œãªã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ï¼‰
        rows = response.xpath('//table//tr')
        for row in rows:
            code = row.xpath('./td[1]/a/text()').get()
            name = row.xpath('./td[2]/a/text()').get()

            if code and name:
                self.total_seen += 1

                nikkei_url = f"https://www.nikkei.com/nkd/company/?scode={code}"
                quick_url = f"https://moneyworld.jp/stock/{code}"
                sbi_url = (
                    f"https://site3.sbisec.co.jp/ETGate/?_ControlID=WPLETsiR001Control"
                    f"&_PageID=WPLETsiR001Idtl10&_DataStoreID=DSWPLETsiR001Control"
                    f"&_ActionID=stockDetail&s_rkbn=2&s_btype=&i_stock_sec={code}&i_dom_flg=1"
                    f"&i_exchange_code=JPN&i_output_type=0&exchange_code=TKY&stock_sec_code_mul={code}"
                    f"&ref_from=1&ref_to=20"
                )

                # è¿½åŠ ï¼ˆINSERT OR IGNOREï¼‰ã—ã€rowcountã§æ–°è¦ç™»éŒ²ã‚’åˆ¤å®š
                self.cursor.execute("""
                    INSERT OR IGNORE INTO consensus_url (
                        target_date, code, name, nikkeiurl, quickurl, sbiurl
                    ) VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    self.target_date, code, name, nikkei_url, quick_url, sbi_url
                ))
                if self.cursor.rowcount == 1:
                    self.total_inserted += 1

        self.conn.commit()

        # ã€Œæ¬¡ã¸ã€ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°ãƒšãƒ¼ã‚¸é·ç§»
        next_page = response.xpath('//a[text()="æ¬¡ã¸"]/@href').get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)
